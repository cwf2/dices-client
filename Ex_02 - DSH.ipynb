{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"well\" style=\"margin:1em 2em\">\n",
    "<p>This Notebook reproduces and expands on a demo from ‚ÄúDistant Reading of Direct Speech in Epic: An Illustrated Workflow,‚Äù a talk I gave at the FIEC / CA annual meeting in London, July 8, 2019.</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Heroes and their moms\n",
    "\n",
    "Let's say we're young scholars interested in Telemachus' speech to Penelope.\n",
    " - How often does he speak to her?\n",
    " - What kind of language does he use?\n",
    " - How does the narrator refer to these speeches?\n",
    " \n",
    "We'll start by showing how the DICES database and Python library can be used to retrieve and manipulate the speeches in question. Then we'll expand our perspective to show how DICES enables research on a \"distant reading\" scale, taking in all heroes and their mothers. Finally, we'll check the accuracy of the automated methods by comparing against a benchmark of hand-curated mother-child speech data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this lets me change the api while the notebook is open\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DICES API\n",
    "\n",
    "See example 1 for notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "api = DicesAPI(\n",
    "    dices_api = 'https://fierce-ravine-99183.herokuapp.com/api',\n",
    "    cts_api = 'http://cts.perseids.org/api/cts/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLTK\n",
    "\n",
    "Make sure the corpora are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "corpora = [\n",
    "    '{}_models_cltk',\n",
    "    '{}_text_perseus',\n",
    "    '{}_treebank_perseus',\n",
    "    '{}_lexica_perseus',\n",
    "]\n",
    "\n",
    "print('Importing corpora:')\n",
    "\n",
    "for lang in ['latin', 'greek']:\n",
    "    downloader = CorpusImporter(lang)\n",
    "    for corpus in corpora:\n",
    "        print(\" - \" + corpus.format(lang))\n",
    "        downloader.import_corpus(corpus.format(lang))\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "tokenizer = {\n",
    "    'greek': WordTokenizer('greek'),\n",
    "    'latin': WordTokenizer('latin'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up lemmatizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.lemmatize.greek.backoff import BackoffGreekLemmatizer\n",
    "lemmatizer = {\n",
    "    'greek': BackoffGreekLemmatizer(),\n",
    "    'latin': BackoffLatinLemmatizer(),    \n",
    "}\n",
    "\n",
    "# regular expressions to tidy up perseus texts for ctlk\n",
    "replacements = {\n",
    "    'greek': [\n",
    "        (r'¬∑', ','),           # FIXME: raised dot? \n",
    "        (chr(700), chr(8217)), # two different apostrophes that look alike\n",
    "    ],\n",
    "    'latin': [\n",
    "        \n",
    "    ],\n",
    "}\n",
    "\n",
    "# compile the regexes\n",
    "for lang in ['greek', 'latin']:\n",
    "    replacements[lang] = [(re.compile(pat), repl) for pat, repl in replacements[lang]]\n",
    "    \n",
    "\n",
    "# generic tokenize-lemmatize function\n",
    "def lemmatize(text, lang):\n",
    "    '''return a set of (token,lemmata) pairs for a string'''\n",
    "    \n",
    "    for pat, repl in replacements[lang]:\n",
    "        text = pat.sub(repl, text)\n",
    "    \n",
    "    tokens = tokenizer[lang].tokenize(text)\n",
    "    lemmata = lemmatizer[lang].lemmatize(tokens)\n",
    "    \n",
    "    return lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "from qwikidata.entity import WikidataItem, WikidataProperty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1\n",
    "\n",
    "Let's start by building a lexicon for all the words Telemachus speaks to Penelope.\n",
    "\n",
    "### Identify and download the speeches\n",
    "\n",
    "Using the hand-rolled DICES API code, we can search speeches using keywords. For now, JSON results from the API are paged, so if your search has a lot of results, you may have to wait for several pages to download. I've added a progress bar widget because I get impatient.\n",
    "\n",
    "Note that I can specify both the speaker and the addressee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = api.getSpeeches(spkr_name='Telemachus', addr_name='Penelope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in speeches:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the passages from a remote library\n",
    "\n",
    "We have the metadata for each speech; now we need the text. The DICES library uses MyCapytain under the hood to retrieve the passages from a remote CTS server: here, Perseus, specified in the `DicesAPI()` call above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "for s in speeches:\n",
    "    cts_passage = s.getCTS()\n",
    "    text = cts_passage.text\n",
    "    passages.append(text)\n",
    "    \n",
    "    print(f'{s.author.name} {s.work.title} {s.l_range}')\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CLTK to parse the text\n",
    "\n",
    "We can use CTLK's tokenizers to break each string into meaningful units -- sentences and/or words. Then we use the backoff lemmatizer to normalize all the inflected forms to dictionary headwords.\n",
    "\n",
    "I rolled these steps into one convenience function up above. üëâüèª *One thing to watch out for is that CTLK needs to know what language you're working with, so I've added a kludge to set language based on the author name; really, that should be built into DICES eventually.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lems = Counter()\n",
    "for p in passages:\n",
    "    lang = s.getLang()\n",
    "    lemmatized = lemmatize(p.lower(), lang)\n",
    "    \n",
    "    these_lems = [lem for tok, lem in lemmatized]\n",
    "    lems.update(these_lems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the counter to a Pandas data frame for tidier presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lems.most_common(), columns=['lemma', 'count'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now let's think more broadly. How typical is this kind of speech? We can use external linked data to find other examples of mother-son conversations in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some custom code to query WikiData\n",
    "\n",
    "This lets us ask whether a given addressee belongs to the set of people having a certain relationship to a given speaker. It takes a while to download the WikiData entities, and I had to run this a number of times, so I cached WD data in the respective character objects once it's downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkWD(c):\n",
    "    '''make sure character has wikidata id'''\n",
    "    if c.char is not None:\n",
    "        if c.char.wd is not None:\n",
    "            if len(c.char.wd.strip()) > 0:\n",
    "                return c.char.wd.strip()\n",
    "\n",
    "def checkWDRelation(s, a, relation, cache=None):\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    else:\n",
    "        if (s.id, a.id) in cache:\n",
    "            return cache[(s.id, a.id)]\n",
    "\n",
    "    res = False\n",
    "\n",
    "    if not hasattr(s, 'wd_ent'):\n",
    "        s.wd_ent = WikidataItem(get_entity_dict_from_api(s.wd))\n",
    "\n",
    "    claim_group = s.wd_ent.get_truthy_claim_group(relation)\n",
    "\n",
    "    for claim in claim_group:\n",
    "        if claim.mainsnak.datavalue is None:\n",
    "            continue\n",
    "        if claim.mainsnak.datavalue.value['id'] == a.wd:\n",
    "            res = True\n",
    "    \n",
    "    cache[(s.id, a.id)] = res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the relation \"mother of\" has the WikiData ID `'P25'`. Here's how we ask if a given addressee is the mother of a given speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = api.getCharacters(name='Telemachus')[0]\n",
    "addressee = api.getCharacters(name='Penelope')[0]\n",
    "\n",
    "print(f'Is {addressee.name} the mother of {speaker.name}?')\n",
    "print(checkWDRelation(speaker, addressee, 'P25'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also added a separate cache just for the boolean result of checkWDRelation, to save a little more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_mothers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using WikiData to filter the speeches\n",
    "\n",
    "The DICES dataset includes WikiData ids for most of the characters (not all). The DICES API doesn't let us query WikiData itself, though. For now, the easiest thing is just to download all the speeches and character IDs, and then cross reference them against WikiData using its own API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all the speeches: takes a minute\n",
    "speeches = []\n",
    "for name in ['Homer', 'Apollonius', 'Virgil']:\n",
    "    speeches.extend(api.getSpeeches(author_name=name, progress=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check each speaker-addressee pair against WikiData**\n",
    "\n",
    "What we actually do here is download the WikiData entity for each speaker, if we don't already have it cached. Then we ask the WD entity for its mom(s), and check the WD ID of the addressee against the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "# create a progress bar\n",
    "pbar = widgets.IntProgress(\n",
    "    value = 0,\n",
    "    min = 0,\n",
    "    max = len(speeches),\n",
    "    bar_style='info',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "pbar_label = widgets.Label(value = f'{pbar.value}/{len(speeches)}')\n",
    "display(widgets.HBox([pbar, pbar_label]))\n",
    "\n",
    "for s in speeches:\n",
    "    if s.spkr is not None and s.addr is not None:\n",
    "        for spkr in s.spkr:\n",
    "            spkr_wd = checkWD(spkr)\n",
    "            if spkr_wd is not None:\n",
    "\n",
    "                for addr in s.addr:\n",
    "                    addr_wd = checkWD(addr)\n",
    "                    if addr_wd is not None:\n",
    "                        df.append((\n",
    "                            s.id,\n",
    "                            s.work.title,\n",
    "                            s.l_fi,\n",
    "                            s.l_la,\n",
    "                            spkr.char.name, spkr_wd, \n",
    "                            addr.char.name, addr_wd,\n",
    "                            checkWDRelation(spkr.char, addr.char, 'P25', cache=cache_mothers),\n",
    "                            checkWDRelation(addr.char, spkr.char, 'P25', cache=cache_mothers)\n",
    "                            ))\n",
    "    pbar.value += 1\n",
    "    pbar_label.value = f'{pbar.value}/{len(speeches)}'\n",
    "\n",
    "df = pd.DataFrame(df, columns=['id', 'work', 'l_first', 'l_last', 'spkr', 'sp_wd', 'addr', 'ad_wd', 'sp_is_mom', 'ad_is_mom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Let's take a look at the results. Here is the complete set of speeches, with the additional attribute `sp_is_mom` if the speaker is the addressee's mother, and `ad_is_mom` if the addressee is the speaker's mother.\n",
    "\n",
    "As a quick sanity check, the first two speeches in the Argonautica, which were at the top of the list when I ran this, are between Jason and his mother, Alcimede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['work']=='Argonautica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to pandas, we can filter the data frame on the new boolean columns to show only speeches between mother and child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = df.loc[df['sp_is_mom'] | df['ad_is_mom'],\n",
    "             ['work', 'l_first', 'l_last', 'spkr', 'addr']]\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also comes in handy if I wanted to export this data to Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Let's see how well the automated approach worked. We'll load up a hand-corrected list of mother-child speeches and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv('data/moms-bench.csv', dtype=str)\n",
    "bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the union of `hits` and `bench` to see how we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hits.merge(bench, on=['work', 'l_first'], how='outer', \n",
    "                        suffixes=['_h', '_b'], indicator=True)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(results[['work', 'l_first', 'spkr_h', 'addr_h', 'spkr_b', 'addr_b', '_merge']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos = sum(results['_merge'] == 'both')\n",
    "\n",
    "p = true_pos / hits.shape[0]\n",
    "r = true_pos / bench.shape[0]\n",
    "\n",
    "print(f'Precision: {p:.2f}')\n",
    "print(f'Recall:    {r:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### The good news\n",
    "Recall was good: we got almost all of the benchmark set, with exceptions to be discussed below. Precision was very high: only a single false positive.\n",
    "\n",
    "#### The bad news\n",
    "First, a brief look at the single false positive: it's a speech in which Aeneas addresses first his father, then his mother, and finally the Trojans around him more generally. Venus isn't there to take place in the conversation, and the words addressed to her could be read as a prayer or a rhetorical apostrophe. I'd say the difference between the database and the benchmark set represents a real difficulty of classification more than a simple false positive.\n",
    "\n",
    "Now, let's look a little more closely at the speeches we missed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = results[results['_merge'] == 'right_only'][\n",
    "                ['work', 'l_first', 'spkr_h', 'addr_h', 'spkr_b', 'addr_b']]\n",
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, I'd say these fall into three groups:\n",
    "\n",
    " 1. A conversation in the Iliad between Hera and a group of gods, some of whom were here children\n",
    " 2. A conversation in the Aeneid between Jupiter and \"Cybele,\" i.e., Rhea.\n",
    " 3. A conversation in the Aeneid between Euryalus and his anonymous mother.\n",
    "\n",
    "#### Digging a little deeper\n",
    "\n",
    "First, let's confirm that all these speeches are in the database results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed.merge(df, how='left', on=['work', 'l_first'])[[\n",
    "    'work', 'l_first',                               # keys: work and locus\n",
    "    'id', 'spkr', 'addr', 'sp_is_mom', 'ad_is_mom',  # cols from df\n",
    "    'spkr_b', 'addr_b'                               # cols from bench\n",
    "    \n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speech between Euyalus and his mom is missing from the database. That's because as of this writing we don't have a systematic way of including anonymous characters like her--folks described only by a family relation or an occupation. Because the character doesn't fit our current data model, she gets omitted, and this speech fails to be added to the database.\n",
    "\n",
    "The conversation between Hera and the gods is there in the database, but the speaker-addressee pairs are not registering as mother-child relationships. In this case, it's because \"gods\" isn't being parsed as including all the individual gods, but rather a corporate entity that doesn't have \"mother\" or \"child\" as properties. This highlights another issue that needs to be resolved in our data model.\n",
    "\n",
    "Finally, the conversation between Jupiter and his mom is also in the database, and each of these characters is matched with a WikiData entity, but we're not getting the right answer about their kinship relation because WikiData has distinct entities for the Greek goddess Rhea and the Phrygian goddess Cybele. We can fix this by pointing the character's WikiData ID to the Greek goddess instead (as we did for the Roman deities), but maybe we should think about the larger problem of poetic ambiguity/metonymy/syncretism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    " - WikiData gave us a lot for free -- all of the individual mother-child relationships were in there when we knew where to look.\n",
    " \n",
    " - There is still some important work to be done refining our underlying data model.\n",
    " \n",
    " - If we want to rely on linked open data for high-stakes work, we need resources that are sensitive to the details we care about. We hope that MANTO, because it's specific to Classical myth and hand-curated by domain experts, can help us with problems like when to treat Cybele and Venus as independent entities and when to consider them identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
